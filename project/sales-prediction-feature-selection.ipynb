{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport sklearn.metrics as metrics\nfrom patsy import dmatrices, dmatrix","metadata":{"execution":{"iopub.status.busy":"2021-09-27T19:31:04.586580Z","iopub.execute_input":"2021-09-27T19:31:04.587564Z","iopub.status.idle":"2021-09-27T19:31:05.818066Z","shell.execute_reply.started":"2021-09-27T19:31:04.587452Z","shell.execute_reply":"2021-09-27T19:31:05.817048Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"Items = pd.read_csv(\"../input/items-ff-final5a/Items_ff_final5 (1).csv\")\nItems.head(1)\n#Items.shape[0]","metadata":{"execution":{"iopub.status.busy":"2021-09-27T19:31:12.394857Z","iopub.execute_input":"2021-09-27T19:31:12.395269Z","iopub.status.idle":"2021-09-27T19:31:21.268293Z","shell.execute_reply.started":"2021-09-27T19:31:12.395217Z","shell.execute_reply":"2021-09-27T19:31:21.267283Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"Items.drop(columns=['description'], inplace=True)\nItems['location'] = Items['location'].astype('category')\nItems['condition'] = Items['condition'].astype('category')\nItems['star_rating'] = Items['star_rating'].astype('category')\nItems['revised_star_rating'] = Items['revised_star_rating'].astype('category')\nItems['sales_cyber_monday'] = Items['sales_cyber_monday'].astype('category')\nItems['sales_black_friday'] = Items['sales_black_friday'].astype('category')\nItems['sales_colombus_day'] = Items['sales_colombus_day'].astype('category')\nItems['sales_labor_day'] = Items['sales_labor_day'].astype('category')\nItems['sales_presidents_day'] = Items['sales_presidents_day'].astype('category')\nItems['sales_4th_july'] = Items['sales_4th_july'].astype('category')\nItems['sales_newyear'] = Items['sales_newyear'].astype('category')\nItems['sales_xms'] = Items['sales_xms'].astype('category')\nItems['success'] = Items['success'].astype('category')\nItems['brand_size'] = Items['brand_size'].astype('category')\nItems['popular_brand'] = Items['popular_brand'].astype('category')\nItems['category_size_rank_q'] = Items['category_size_rank_q'].astype('category')\nItems['review_count_log'] = Items['review_count_log'].astype('category')\nItems['price_diff_competitor_log'] = Items['price_diff_competitor_log'].astype('category')\nItems['profit_pct_sqrt'] = Items['profit_pct_sqrt'].astype('category')\nItems['reviews_cnt_category'] = Items['reviews_cnt_category'].astype('category')\nItems['creation_time'] = Items['creation_time'].astype('datetime64')\nItems.columns","metadata":{"execution":{"iopub.status.busy":"2021-09-27T19:31:21.270570Z","iopub.execute_input":"2021-09-27T19:31:21.270922Z","iopub.status.idle":"2021-09-27T19:31:21.848458Z","shell.execute_reply.started":"2021-09-27T19:31:21.270881Z","shell.execute_reply":"2021-09-27T19:31:21.847602Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# Text analysis\n**item description** feature analysis\n\nThis code was intended to be part of the DATA ENRICHMENT notebook but was added late and presented here.","metadata":{}},{"cell_type":"code","source":"text_df =  pd.read_csv(\"../input/text-analysis-final/text_analysis_2\")","metadata":{"execution":{"iopub.status.busy":"2021-09-27T19:31:21.849798Z","iopub.execute_input":"2021-09-27T19:31:21.850114Z","iopub.status.idle":"2021-09-27T19:31:26.006978Z","shell.execute_reply.started":"2021-09-27T19:31:21.850075Z","shell.execute_reply":"2021-09-27T19:31:26.005989Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"add char_count, avg_words, stopwords, numerics count, upper case letters and Sentiment Analysis","metadata":{}},{"cell_type":"code","source":"text_df.head()\ntext_df.dtypes\ntext_df.drop(['Unnamed: 0', 'text_df'], axis = 1, inplace = True)\nItems = pd.concat([Items, text_df], axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-09-27T19:31:26.009434Z","iopub.execute_input":"2021-09-27T19:31:26.009687Z","iopub.status.idle":"2021-09-27T19:31:26.070699Z","shell.execute_reply.started":"2021-09-27T19:31:26.009658Z","shell.execute_reply":"2021-09-27T19:31:26.069915Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"  \n\"\"\"\nanalysis                                                        ####\nAuthor: Tomas Karpati M.D.                                      ####\nCreation date: 2019-01-02                                       ####\nLast Modified: 2020-02-28                                       ####\n\"\"\"\n\n__author__ = \"Tomas Karpati <karpati@it4biotech.com>\"\n__version__ = \"0.1.3\"\n\n\"\"\"\nTable1\nUsage:\ndata: a pandas dataframe\nx: character list with the name of the variables\ny: the name of the strata variable (optional)\nrn: character list with the text we want to be used to desxribe the variable names\nmiss: include missing statistics: [0=none, 1=only for categorical variables, 2=for all variables]\ncatmiss: On categorical variables, adds a new category (Missing) to the available\ncategories. Default is FALSE. For activation change this to TRUE. If TRUE, the \"miss\"\nparameter will not be used for category variables.\nformatted: As default, the table output is formatted as text with values that\ninclude parenthesis and percentages, e.g. 153 (26.5\\%). If you are interested that the\ntable return each numeric value as aseparate cell, set this variable to FALSE.\ncategorize: If there are categorical variables that are defined as numeric we can\nforce the function to take them as categorical (factor) by changing this to TRUE. Default\nis FALSE.\nfactorVars: If categorize is set to TRUE, a list of variables to be considered as\ncategoricals may be given. In this case, maxcat will not be used and only the variables\nin the list will be converted to factors.\nmaxcat: If we force categorize to be TRUE, maxcat will be used to define the\nmaximum number of unique values permited for a variable to be considered as categorical.\nDefault is 10.\ndecimals: Determinate the number of decimal places of numerical data. Default is 1\nmessages: This switch will show the iterations of the function through the variables. In case you want to suppress those message set it to FALSE.\nexcel: export the table to excel [0=no, 1=yes]\nexcel_file: the name of the excel file we want to save the table (optional)\n\"\"\"\n\nimport time\nimport sys\nimport os\nimport numpy as np\nfrom scipy import stats\n#from statsmodels.stats import multitest\nfrom statsmodels.formula.api import ols\n#import statsmodels.stats.api as sms\nfrom statsmodels.stats.anova import anova_lm\nimport pandas as pd\nfrom sklearn.preprocessing import normalize\n#import matplotlib\n### preventing matplotlib to open a graph window when saving...\n#matplotlib.use('Agg')\nimport matplotlib.pyplot as plt\n### preventing matplotlib to open a graph window when saving...\nplt.ioff()\nimport seaborn as sns\n## dissable the \"SettingWithCopyWarning\" warning\npd.options.mode.chained_assignment = None  # default='warn'\n#import warnings\n\nif(pd.__version__ < '0.21.0'):\n    pd.set_option('use_inf_as_null',True)\nelse:\n    pd.set_option('use_inf_as_na',True)\n\n    \ndef Table1(data=None, x='',y='', rn='', miss=True, catmiss=True, formatted=True, categorize=True, factorVars='', maxcat=6, decimals=1, messages=True, excel=False, excel_file=''):\n        data.head()\n        print(\"Begining analysis...\")\n        table1 = _getTable1(data,x,  y, rn, miss, catmiss, formatted, categorize, factorVars, maxcat, decimals, messages, excel, excel_file)\n    #    #self.table1 = self._getTable1(x, data, y, rn, miss, catmiss, formatted, categorize, factorVars, maxcat, decimals, messages, excel, excel_file)\n        return(table1)\n\ndef _g1(var):\n        res = {'mean':np.nanmean(var), 'sd':np.nanstd(var)}\n        return(res)\n\ndef _g2(var):\n        res = {'median':np.nanmedian(var), 'irq_25':np.nanpercentile(var,25), 'irq_75':np.nanpercentile(var,75)}\n        return(res)\n\ndef _getUniqueCount(data):\n        import pandas as pd\n        bb = data.columns.tolist()\n        cc = {}\n        for v in bb:\n            cc[v] = len(data.groupby(v).count())\n        return(pd.Series(cc))\n\n\ndef _to_categorical(x):\n        x = x.astype('category')\n        return(x)\n\ndef _setFactors(data, factorVars, unq, catmiss, maxcat):\n        aa =data.dtypes\n        if(len(factorVars) > 0):\n            for v in factorVars:\n                #print(\"Variable %s is a %s\" % (v,aa[v].name))\n                if(aa[v].name!='category'):\n                    data.loc[:,v] = _to_categorical(data[v])\n                if(catmiss==True):\n                    if(data[v].isnull().sum()>0):\n                        #print(\"Adding missing category to %s\" % v)\n                        data[v] = data[v].cat.add_categories(['Missing'])\n                        data.loc[data[v].isnull(),v] = \"Missing\"\n\n        elif(len(factorVars)==0):\n            #factorVars = self._getUniqueCount(data)\n            factorVars = unq\n            factorVars = factorVars[factorVars <= maxcat]\n            for v in factorVars.index:\n                if(aa[v].name!=\"category\"):\n                    data.loc[:,v] = _to_categorical(data[v])\n                if(catmiss==True):\n                    if(data[v].isnull().sum()>0):\n                        data[v].cat.add_categories(['Missing'])\n                        data.loc[data[v].isnull(),v] = \"Missing\"\n        return(data)\n\n\ndef _getSimpleTable(data, x, rn, miss, catmiss, formatted, categorize, factorVars, unq, maxcat, decimals, messages):\n        msg=[]\n        if (len(rn)==0):\n            rn = x\n        ln = len(x)\n        ### init the progress bar\n        sys.stdout.write(\"[%s]\" % (\"\" * ln))\n        sys.stdout.flush()\n        sys.stdout.write(\"\\b\" * (ln+1)) # return to start of line, after '['\n        toolbar_width = 80\n        ### define the column names\n        tableaaaa = [[0,\"Individuals\",\"n\",1, len(data)]]\n        tablebbbb = [[0,\"Individuals\",\"n\",1, len(data),'','']]\n        q = 0\n        n = 0\n        ii = 0\n        tm = 0\n        for v in x:\n          time.sleep(0.1) # do real work here\n          # update the progress bar\n          sys.stdout.write(\"*\")\n          sys.stdout.flush()\n          ### check if the variable name exists in the dataset\n          if({v}.issubset(data.columns)):\n            ### check  if there are no values on the variable\n            #if(data[v].nunique()==0):\n            if(unq[v]==0):\n                msg.append(\"The variable %s has no data... avoided\" % v)\n            ### define if the actual variable has to be treated as numeric or factor\n            aa = data.dtypes\n            #if(categorize==True and data[v].nunique() <= maxcat):\n            if(categorize==True and (unq[v] <= maxcat)):\n                data.loc[:,v] = _to_categorical(data[v])\n            ### If date/time, don't show\n            if(aa[v].name == 'datetime64[ns]'):\n                msg.append(\"The variable %s is a date. Dates are not allowed in Table1... avoided\" % v)\n            ### if it is defined as object (not assigned numerical or categorical), ignore\n            elif(aa[v].name == 'object'):\n                msg.append(\"The variable %s is not well defined. This data type is not allowed in Table1... avoided\" % v)\n            ### if it is numeric, show\n            elif(aa[v].name == 'float64' or aa[v].name == 'int64'):\n                ## report mean and standard deviation\n                t_n = _g1(data[v])\n                tp = \"%s (%s)\" % ('{:8,.2f}'.format(round(t_n['mean'],decimals)), '{:8,.2f}'.format(round(t_n['sd'],decimals)))\n                tbl1 = [0, rn[q],\"Mean (SD)\",1, tp]\n                tbl2 = [0, rn[q],\"Mean (SD)\",1, round(t_n['mean'],5),round(t_n['sd'],5),'']\n                tableaaaa.append(tbl1)\n                tablebbbb.append(tbl2)\n                ## report median and Interquartile ranges (25%,75%)\n                t_n = _g2(data[[v]])\n                tp = \"%s (%s-%s)\" % ('{:8,.2f}'.format(round(t_n['median'],decimals)), '{:8,.2f}'.format(round(t_n['irq_25'],decimals)), '{:8,.2f}'.format(round(t_n['irq_75'],decimals)))\n                tbl1 = [0, rn[q],\"Median (IQR)\",2, tp]\n                tbl2 = [0, rn[q],\"Median (IQR)\",2, round(t_n['median'],5),round(t_n['irq_25'],5),round(t_n['irq_75'],5)]\n                tableaaaa.append(tbl1)\n                tablebbbb.append(tbl2)\n                ## report number and percent of missing\n                if (miss >= 1):\n                    if (data[v].isnull().sum()>0):\n                        t_n  = len(data)\n                        t_m = data[v].isnull().sum()\n                        tp = \"%s (%s%%)\" % ('{:8,.2f}'.format(t_m),'{:8,.2f}'.format(round((t_m/t_n)*100,decimals)))\n                        tbl1 = [0,rn[q],\"Missing (%)\",3, tp]\n                        tbl2 = [0,rn[q],\"Missing (%)\",3, t_m, (t_m/t_n)*100, ]\n                    else:\n                        tbl1 = [1,rn[q],\"Missing (%)\",3, \" -- \"]\n                        tbl2 = [1,rn[q],\"Missing (%)\",3,'' ,'' ,'' ]\n                    tableaaaa.append(tbl1)\n                    tablebbbb.append(tbl2)\n            elif(aa[v].name == \"category\"):\n                #if(data[v].nunique()>8):\n                if(len(data.groupby(v).count())>8):\n                    tmpcat = pd.Series.value_counts(data[v],dropna=(not catmiss))\n                    n = len(tmpcat)\n                    if(n > 8):\n                        v1 = tmpcat[0:6].values\n                        v2 = np.append(v1,tmpcat[7:n].sum())\n                        a1 = tmpcat.index[0:6].values.tolist()\n                        a2 = a1.extend(['Other'])\n                        t_n = pd.Series(v2,a1)\n                else:\n                    t_n = pd.Series.value_counts(data[v],dropna=(not catmiss))\n                ttotal = len(data)\n                #nm = data[v].unique()\n                nm = t_n.index.values\n                for f in range(0,len(nm)):\n                    del1 = 0\n                    if(len(nm)==2 and (nm[f]==\"No\" or nm[f]==\"no\" or nm[f]==0 or nm[f]==\"0\" or nm[f]==\"None\" or nm[f]==\"none\")):\n                        del1 = 1\n                    tp = t_n.iloc[f] / ttotal * 100\n                    pct = \"%s (%s%%)\" % ('{:8,.2f}'.format(round(t_n.iloc[f],decimals)), '{:8,.2f}'.format(round(tp,decimals)))\n                    tbl1 = [del1,rn[q],nm[f],f, pct]             ########### delete rows 0/1 !!!!!!!!!\n                    tbl2 = [del1,rn[q],nm[f],f, t_n.iloc[f], tp, ]    ########### delete rows 0/1 !!!!!!!!!\n                    tableaaaa.append(tbl1)\n                    tablebbbb.append(tbl2)\n                if (miss >= 2 and catmiss==False ):\n                    if (data[v].isnull().sum()>0):\n                      t_n = len(data)\n                      t_m = data[v].isnull().sum()\n                      tp = \"%s (%s%%)\" % ('{:8,.2f}'.format(t_m), '{:8,.2f}'.format(round((t_m/t_n)*100,decimals)))\n                      tbl1 = [0, rn[q], \"Missing (%)\", f, tp]\n                      tbl2 = [0, rn[q], \"Missing (%)\", f, t_m, (t_m/t_n)*100, ]\n                    else:\n                      tbl1 = [1,rn[q],\"Missing (%)\",f, \" -- \"]\n                      tbl2 = [1,rn[q],\"Missing (%)\",f,'' ,'' , '']\n                    tableaaaa.append(tbl1)\n                    tablebbbb.append(tbl2)\n            else:\n                msg.append(\"The variable %s doesn't exists in the dataset... avoiding\" % v)\n\n            q = q + 1\n            ii = ii + 1\n            tm = tm + 1\n        if(formatted==True):\n          ### terminate the progress bar\n          sys.stdout.write(\"\\n\")\n          if(messages==True):\n              print(msg)\n          return(tableaaaa)\n        else:\n          ### terminate the progress bar\n          sys.stdout.write(\"\\n\")\n          if(messages==True):\n              print(msg)\n          return(tablebbbb)\n\n\ndef _pvals(x, y, rn, data, unq, messages):\n        msg = []\n        ptab = [] #[[\"Variables\",\"pval\", \"n\"]]\n        if (y!=''):\n          if ({y}.issubset(data.columns)):\n            if (len(rn)==0 or len(rn)<2):\n                rn = x\n            q = 0\n            for v in x:\n              #print(v)\n              if ({v}.issubset(data.columns) and v != y):\n                #factorY = data[y].nunique()\n                factorY = unq[y]\n                aa = data.dtypes\n                if((aa[y].name == 'float64' or aa[y].name == 'int64' or aa[y].name == 'object') and factorY <= maxcat):\n                    data.loc[:,y] = to_categorical(data[y])\n                elif (aa[y].name == 'float64' or aa[y].name == 'int64'):\n                  msg.append(\"The variable %s is not a factor. Please convert to factor or change the 'categorize' flag to TRUE.\" % y)\n                  pval = []\n\n                #if ((aa[v].name == 'float64' or aa[v].name == 'int64') and (data[y].nunique() > 1)):\n                if ((aa[v].name == 'float64' or aa[v].name == 'int64' or aa[v].name == 'float32' or aa[v].name == 'int32') and (unq[y] > 1)):\n                  ### first check for homoscedasticity\n                    bb = pd.Series({x : y.tolist() for x,y in data[v].groupby(data[y])})\n                    if (stats.bartlett(*bb)[1] >= 0.05):\n                        formula = \"%s ~ %s\" % (v,y)\n                        model = ols(formula,data=data,missing='drop').fit()\n                        aov_table = anova_lm(model, typ=2)\n                        pval =round(aov_table['PR(>F)'][0],3)\n                    else:\n                        prevar = data.loc[-data[v].isnull()]\n                        bb = pd.Series({x : y.tolist() for x,y in prevar[v].groupby(prevar[y])})\n                        pval = stats.f_oneway(*bb)[1]\n                #elif (data[y].nunique()==1):\n                elif (unq[y]==1):\n                  pval = np.nan\n                elif (aa[v].name==\"datetime64[ns]\"):\n                    pval = np.nan\n                else:\n                    if(pd.crosstab(data[v],data[y]).min().min()>5):\n                        pval = stats.chi2_contingency(pd.crosstab(data[v],data[y]))[1]\n                    else:\n                        ct = pd.crosstab(data[v],data[y])\n                        if(ct.min().min()==0):\n                        # in cases where there are cells with zero, we use Fisher's exact test\n                            if(ct.shape == (2,2)):\n                                pval = stats.fisher_exact(ct)[1]\n                            else:\n                                pval = stats.chi2_contingency(pd.crosstab(data[v],data[y]))[1]\n                                msg.append(\"Unable to calcualte the Fisher exact test for variables %s and %s... The p-value may be incorrect\" % (v,y))\n                        else:\n                            pval = stats.mstats.kruskalwallis(pd.crosstab(data[v],data[y]))[1]\n                ptab.append([rn[q],round(pval,3),1])\n              q = q + 1\n        if(messages==True):\n            print(msg)\n        return(ptab)\n\n###############################################################################################\n####################### Begin analysis\n###############################################################################################\n\ndef _getTable1(data, x, y, rn, miss, catmiss, formatted, categorize, factorVars, maxcat, decimals, messages, excel, excel_file):\n        import time\n        init = time.time()\n\n        print(\"Factorizing... please wait\")\n        if (x=='' or len(x)==0):\n            x = data.columns.tolist()\n        if(y!='' and {y}.issubset(x)):\n            #print(x)\n            x.remove(y)\n            #if ({'Unnamed: 0'}.issubset(x)):\n            #    x.drop('Unnamed: 0')\n        unq = _getUniqueCount(data)\n        if ({'Unnamed: 0'}.issubset(unq)):\n            unq.drop('Unnamed: 0')\n        if (len(factorVars)==0):\n            factorVars = unq[unq <= maxcat].index\n        #print(data.dtypes)\n        data = _setFactors(data=data, factorVars=factorVars, unq=unq, catmiss=catmiss, maxcat=maxcat)\n        #print(data.dtypes)\n        ##### if y is null then make a simple table\n        #print(\"_getSimpleTable pass 1...\")\n        tabaaa1 = _getSimpleTable(x=x, rn=rn, data=data, miss=miss, catmiss=catmiss, unq=unq, formatted=formatted, categorize=categorize, factorVars=factorVars, maxcat=maxcat, decimals=decimals, messages=messages)\n        if(formatted==True):\n            tabaaa1 = pd.DataFrame(tabaaa1,columns=[\"Del\",\"Variables\",\"Categories\",\"n\",\"Population\"])\n        else:\n            tabaaa1 = pd.DataFrame(tabaaa1,columns=[\"Del\",\"Variables\",\"Categories\",\"n\",\"val1\",\"val2\",\"val3\"])\n        #print(tabaaa1)\n        ##### if y has two levels, then make a compound comparison\n        if (y!=''): #1\n            if ({y}.issubset(data.columns)):  #2\n                if (data[y].dtype == \"category\"): #3\n                    if (unq[y] > 8): #4\n                        if (messages==True): #5\n                            print(\"The dependent variable has more than 8 levels, table too large!\")\n                    elif(min(pd.Series.value_counts(data[y]))==0): #4\n                        print(\"The dependent variable has one or more levels with no items assigned!\")\n                    else: # 4\n                        data.loc[:,y] = _to_categorical(data[y])\n                #if (data[y].nunique() >= 2): #3\n                if (unq[y] > 6): #3\n                    print(\"You have selected a Y that has more than six different values...\")\n                elif (unq[y] >= 2 and unq[y]<=6): #3\n                    for lv in data[y].unique(): #4\n                        #print(\"Category %s\" % lv)\n                        dtsub = data.loc[data[y]==lv]\n                        #print(\"_getSimpleTable Y pass ...\")\n                        tab = _getSimpleTable(x=dtsub.columns,data=dtsub, rn=rn, miss=miss, unq=unq, catmiss=catmiss, formatted=formatted, categorize=categorize, factorVars=factorVars, maxcat=maxcat, decimals=decimals, messages=False)\n                        if(formatted==True): # 5\n                            tab1 = pd.DataFrame(tab,columns=[\"Del\",\"Variables\",\"Categories\",\"n\",'Category_%s' % lv])\n                        else: #5\n                            tab1 = pd.DataFrame(tab,columns=[\"Del\",\"Variables\",\"Categories\",\"n\",\"Cat_%s_val1\" % lv,\"Cat_%s_val2\" % lv,\"Cat_%s_val3\" % lv])\n                        tab1 = tab1.drop(['n'], axis=1) #5\n                        #print(tab)\n                        tabaaa1 = pd.merge(tabaaa1, tab1, on=['Del','Variables','Categories'],how='left')\n                    #print(tabaaa1)\n\n                    #print(\"_pvals pass ...\")\n                    ptab = _pvals(x=x,y=y, rn=rn, data=data, unq=unq, messages=messages)\n                    ptab = pd.DataFrame(ptab,columns=[\"Variables\",\"p_value\", \"n\"])\n                    #print(ptab)\n                    tabaaa1 = pd.merge(tabaaa1, ptab, on=['Variables','n'],how='left')\n                    tabaaa1 = tabaaa1.loc[tabaaa1['Population']!=\" -- \"]\n                    tabaaa1 = tabaaa1.drop('n',1)\n                    tabaaa1 = tabaaa1.drop('Del',1)\n        #### Save as excel file\n        if(excel==True):\n            if(excel_file == ''):\n                print(\"Please fill in the <excel_file> parameter with the file name including the path.\")\n            else:\n                writer = pd.ExcelWriter(excel_file)\n                tabaaa1.to_excel(writer,'Table1',index=False)\n                writer.save()\n                print(\"Excel file written to %s\" % excel_file)\n        print(\"------ Finished in % seconds -----\" % (time.time() - init))\n        return(tabaaa1) #1\n\n    \n############################################################################\n#####   TEST & TRAIN DATASET GENERATION                                 ####\n#####   Author: Tomas Karpati M.D.                                      ####\n#####   Creation date: 2016-08-17                                       ####\n############################################################################\n\n\"\"\"\ntrain_test\nGenerates a training and test dataset and checks if it is well balanced\ndata: original dataset\nprop: the proportion of the training dataset. The value is a fractional number between 0 and 1. The value default value is set to 0.7, indicating that the training dataset will contain 60% of the cases and the test dataset will contain the 40% of the cases.\nseed: the desired seed. Using a constant seed value allows to obtain the same individuals on each group when running many times (important feature needed for replicability)\ntableone: a logical value indicating if the Table1 function has to be generated for comparing the train and test division. Default is FALSE \n\"\"\"\n\n\ndef train_test(data=None,prop=0.7,seed=1,tableone=False):\n    import numpy as np\n    from sklearn.model_selection import train_test_split\n    \n    train, test = train_test_split(data, test_size=1-prop, random_state=seed)\n    dtr = train\n    dte = test\n    dtr['split'] = 'train'\n    dte['split'] = 'test'\n    dd = dtr.append(dte)\n    \n    tab1 = Table1(dd,y='split')\n    vn = tab1.loc[tab1['p_value']<0.05]\n    vn = vn['Variables'].tolist()\n    if(len(vn) == 0):\n        print(\" \")\n        print(\"You got a perfectly balanced training and test datasets\")\n        print(\" \")\n    else:\n        print(\"WARNING: The following variables are not balanced between the training and test datasets:\")\n        print(vn)\n        print(\"You can try to change the seed value until you get a balanced partition.\")\n        print(\"Alternatively, you can ommit this warning and exclude those variables from your model\")\n        print(\" \")\n    \n    if(tableone == True):\n        print(tab1)\n    return([train,test])","metadata":{"execution":{"iopub.status.busy":"2021-09-27T19:31:26.072332Z","iopub.execute_input":"2021-09-27T19:31:26.072992Z","iopub.status.idle":"2021-09-27T19:31:26.354856Z","shell.execute_reply.started":"2021-09-27T19:31:26.072947Z","shell.execute_reply":"2021-09-27T19:31:26.353670Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"##############################","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import mean_absolute_error\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.datasets import make_classification","metadata":{"execution":{"iopub.status.busy":"2021-09-27T19:31:26.356487Z","iopub.execute_input":"2021-09-27T19:31:26.356842Z","iopub.status.idle":"2021-09-27T19:31:26.816618Z","shell.execute_reply.started":"2021-09-27T19:31:26.356803Z","shell.execute_reply":"2021-09-27T19:31:26.815691Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def classificationMetrics(y, yhat):\n    prf1 = metrics.precision_recall_fscore_support(y,yhat)\n    res = {'Accuracy': metrics.accuracy_score(y,yhat),\n           'Precision':prf1[0][1],\n           'Recall': prf1[1][1],\n           'f1-score': prf1[2][1],\n           'Log-loss': metrics.log_loss(y,yhat),\n           'AUC': metrics.roc_auc_score(y,yhat)\n          }\n    return res\n\n","metadata":{"execution":{"iopub.status.busy":"2021-09-27T19:31:26.818089Z","iopub.execute_input":"2021-09-27T19:31:26.818355Z","iopub.status.idle":"2021-09-27T19:31:26.824552Z","shell.execute_reply.started":"2021-09-27T19:31:26.818311Z","shell.execute_reply":"2021-09-27T19:31:26.823707Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"##############################","metadata":{}},{"cell_type":"code","source":"tab1 = Table1(data=Items, y=\"success\")","metadata":{"execution":{"iopub.status.busy":"2021-09-27T19:31:26.825852Z","iopub.execute_input":"2021-09-27T19:31:26.826091Z","iopub.status.idle":"2021-09-27T19:39:13.034071Z","shell.execute_reply.started":"2021-09-27T19:31:26.826064Z","shell.execute_reply":"2021-09-27T19:39:13.032900Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"tab1[tab1['p_value']<0.05]","metadata":{"execution":{"iopub.status.busy":"2021-09-27T19:39:13.037209Z","iopub.execute_input":"2021-09-27T19:39:13.037901Z","iopub.status.idle":"2021-09-27T19:39:13.065064Z","shell.execute_reply.started":"2021-09-27T19:39:13.037833Z","shell.execute_reply":"2021-09-27T19:39:13.064098Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"vn1 = tab1.loc[tab1['p_value']<0.05,'Variables'].unique()\nprint(len(vn1))\nvn1","metadata":{"execution":{"iopub.status.busy":"2021-09-27T19:39:13.069530Z","iopub.execute_input":"2021-09-27T19:39:13.069936Z","iopub.status.idle":"2021-09-27T19:39:13.084743Z","shell.execute_reply.started":"2021-09-27T19:39:13.069903Z","shell.execute_reply":"2021-09-27T19:39:13.083301Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"cols= Items.columns\nvarSel = pd.DataFrame(cols, columns = ['Variable'])\nvarSel = varSel.drop([0,15], axis=0)\nvarSel['Univariable'] = 0\nvarSel.loc[varSel['Variable'].isin(vn1), 'Univariable'] = 1\nvarSel","metadata":{"execution":{"iopub.status.busy":"2021-09-27T19:39:13.086493Z","iopub.execute_input":"2021-09-27T19:39:13.086747Z","iopub.status.idle":"2021-09-27T19:39:13.107895Z","shell.execute_reply.started":"2021-09-27T19:39:13.086718Z","shell.execute_reply":"2021-09-27T19:39:13.107082Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"## Multivariable Analysis","metadata":{}},{"cell_type":"code","source":"## remove unnecessary vars\nX = Items\nX = X.drop(['creation_time','success'], axis = 1)\nX.head()\nX.shape","metadata":{"execution":{"iopub.status.busy":"2021-09-27T19:39:13.109326Z","iopub.execute_input":"2021-09-27T19:39:13.109834Z","iopub.status.idle":"2021-09-27T19:39:13.138277Z","shell.execute_reply.started":"2021-09-27T19:39:13.109799Z","shell.execute_reply":"2021-09-27T19:39:13.137619Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"y = Items.iloc[:,15:16]\ny.head()\nprint([X.shape,y.shape])","metadata":{"execution":{"iopub.status.busy":"2021-09-27T19:39:13.139556Z","iopub.execute_input":"2021-09-27T19:39:13.140005Z","iopub.status.idle":"2021-09-27T19:39:13.145847Z","shell.execute_reply.started":"2021-09-27T19:39:13.139974Z","shell.execute_reply":"2021-09-27T19:39:13.145181Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"### Variable Selection using LASSO (L1 penalization)","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import Lasso\nfrom sklearn.feature_selection import SelectFromModel\n\nlassomod = Lasso(alpha=0.01).fit(X, y)","metadata":{"execution":{"iopub.status.busy":"2021-09-27T19:39:13.147128Z","iopub.execute_input":"2021-09-27T19:39:13.147521Z","iopub.status.idle":"2021-09-27T19:39:14.169795Z","shell.execute_reply.started":"2021-09-27T19:39:13.147493Z","shell.execute_reply":"2021-09-27T19:39:14.168884Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"model = SelectFromModel(lassomod, prefit=True)\nmodel.get_support()","metadata":{"execution":{"iopub.status.busy":"2021-09-27T19:39:14.171206Z","iopub.execute_input":"2021-09-27T19:39:14.171523Z","iopub.status.idle":"2021-09-27T19:39:14.202290Z","shell.execute_reply.started":"2021-09-27T19:39:14.171486Z","shell.execute_reply":"2021-09-27T19:39:14.197926Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"#varSel = varSel.drop(labels=[0,1], axis=0)\nvarSel['Lasso'] = model.get_support().astype('int64')\nvarSel","metadata":{"execution":{"iopub.status.busy":"2021-09-27T19:39:14.203468Z","iopub.execute_input":"2021-09-27T19:39:14.203735Z","iopub.status.idle":"2021-09-27T19:39:14.227104Z","shell.execute_reply.started":"2021-09-27T19:39:14.203700Z","shell.execute_reply":"2021-09-27T19:39:14.226234Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"### Variable Selection using Random Forest\n\n---","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.feature_selection import SelectFromModel\n\nrfmod = RandomForestClassifier(random_state=1).fit(X, y)\n","metadata":{"execution":{"iopub.status.busy":"2021-09-27T19:39:14.228459Z","iopub.execute_input":"2021-09-27T19:39:14.229963Z","iopub.status.idle":"2021-09-27T19:40:52.654507Z","shell.execute_reply.started":"2021-09-27T19:39:14.229914Z","shell.execute_reply":"2021-09-27T19:40:52.653467Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"varSel['RandomForest'] = model.get_support().astype('int64')\nvarSel","metadata":{"execution":{"iopub.status.busy":"2021-09-27T19:40:52.655987Z","iopub.execute_input":"2021-09-27T19:40:52.656227Z","iopub.status.idle":"2021-09-27T19:40:52.673480Z","shell.execute_reply.started":"2021-09-27T19:40:52.656199Z","shell.execute_reply":"2021-09-27T19:40:52.672591Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"### Variable Selection using Gradient Boosting classification","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.feature_selection import SelectFromModel\n\ngbmod = GradientBoostingClassifier(random_state=1).fit(X, y)","metadata":{"execution":{"iopub.status.busy":"2021-09-27T19:40:52.674961Z","iopub.execute_input":"2021-09-27T19:40:52.676056Z","iopub.status.idle":"2021-09-27T19:44:44.773390Z","shell.execute_reply.started":"2021-09-27T19:40:52.676008Z","shell.execute_reply":"2021-09-27T19:44:44.772314Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"model = SelectFromModel(gbmod, prefit=True)\nmodel.get_support()","metadata":{"execution":{"iopub.status.busy":"2021-09-27T19:44:44.774922Z","iopub.execute_input":"2021-09-27T19:44:44.775223Z","iopub.status.idle":"2021-09-27T19:44:44.784137Z","shell.execute_reply.started":"2021-09-27T19:44:44.775190Z","shell.execute_reply":"2021-09-27T19:44:44.783571Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"varSel['GradientBoost'] = model.get_support().astype('int64')\nvarSel","metadata":{"execution":{"iopub.status.busy":"2021-09-27T19:44:44.785183Z","iopub.execute_input":"2021-09-27T19:44:44.785874Z","iopub.status.idle":"2021-09-27T19:44:44.813381Z","shell.execute_reply.started":"2021-09-27T19:44:44.785837Z","shell.execute_reply":"2021-09-27T19:44:44.812765Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"### Summarization and Selection of Variables ","metadata":{}},{"cell_type":"code","source":"varSel['Sum'] =  np.sum(varSel,axis=1)\nvarSel","metadata":{"execution":{"iopub.status.busy":"2021-09-27T19:44:44.814437Z","iopub.execute_input":"2021-09-27T19:44:44.815231Z","iopub.status.idle":"2021-09-27T19:44:44.845420Z","shell.execute_reply.started":"2021-09-27T19:44:44.815195Z","shell.execute_reply":"2021-09-27T19:44:44.844590Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"varSel.groupby('Sum')['Variable'].count()","metadata":{"execution":{"iopub.status.busy":"2021-09-27T19:44:44.846761Z","iopub.execute_input":"2021-09-27T19:44:44.847006Z","iopub.status.idle":"2021-09-27T19:44:44.857920Z","shell.execute_reply.started":"2021-09-27T19:44:44.846976Z","shell.execute_reply":"2021-09-27T19:44:44.856839Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"varSel[varSel['Sum']>=2].iloc[:, 0]\nfinal_vars = varSel[varSel['Sum']>=2].iloc[:, 0]\nfinal_vars","metadata":{"execution":{"iopub.status.busy":"2021-09-27T19:44:44.859903Z","iopub.execute_input":"2021-09-27T19:44:44.860568Z","iopub.status.idle":"2021-09-27T19:44:44.876718Z","shell.execute_reply.started":"2021-09-27T19:44:44.860516Z","shell.execute_reply":"2021-09-27T19:44:44.875755Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"# Partition","metadata":{}},{"cell_type":"code","source":"final_vars.reset_index()\nfinal_vars[11] = 'success'\nItems1 = Items[final_vars]","metadata":{"execution":{"iopub.status.busy":"2021-09-27T20:00:11.550633Z","iopub.execute_input":"2021-09-27T20:00:11.551418Z","iopub.status.idle":"2021-09-27T20:00:11.573131Z","shell.execute_reply.started":"2021-09-27T20:00:11.551370Z","shell.execute_reply":"2021-09-27T20:00:11.571993Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"temp = train_test(data=Items1,prop=0.7,seed=2,tableone=True)","metadata":{"execution":{"iopub.status.busy":"2021-09-27T20:00:17.751473Z","iopub.execute_input":"2021-09-27T20:00:17.751833Z","iopub.status.idle":"2021-09-27T20:01:11.362217Z","shell.execute_reply.started":"2021-09-27T20:00:17.751800Z","shell.execute_reply":"2021-09-27T20:01:11.361394Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"test = temp[1]\ntrain = train_test(data=temp[0],prop=0.8,seed=2,tableone=True)\ndev = train[1]\ntrain = train[0]","metadata":{"execution":{"iopub.status.busy":"2021-09-27T20:01:11.363901Z","iopub.execute_input":"2021-09-27T20:01:11.364187Z","iopub.status.idle":"2021-09-27T20:01:50.234963Z","shell.execute_reply.started":"2021-09-27T20:01:11.364154Z","shell.execute_reply":"2021-09-27T20:01:50.233888Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"import pickle\npickle.dump( train, open( \"./train\", \"wb\" ))\npickle.dump( dev, open( \"./dev\", \"wb\" ))\npickle.dump( test, open( \"./test\", \"wb\" ))","metadata":{"execution":{"iopub.status.busy":"2021-09-27T20:01:50.236394Z","iopub.execute_input":"2021-09-27T20:01:50.236669Z","iopub.status.idle":"2021-09-27T20:01:50.320933Z","shell.execute_reply.started":"2021-09-27T20:01:50.236635Z","shell.execute_reply":"2021-09-27T20:01:50.319892Z"},"trusted":true},"execution_count":30,"outputs":[]}]}